import numpy as np
import json
import glob
import copy
import os.path
import re
from sklearn.feature_extraction import FeatureHasher


def read_label_csv(path):
    label_table = dict()
    with open(path, "r", encoding='ISO-8859-1') as f:
        for line in f.readlines()[1:]:
            fname, label = line.strip().split(",")
            label_table[fname] = int(label)
    return label_table


def read_json(path):
    with open(path, "r") as f:
        return json.load(f)


def parse_set(path):
    ret = set()
    f = open(path, "r")
    lines = f.readlines()
    f.close()
    for line in lines:
        ret.add(line.rstrip())
    return ret


class PeminerParser:
    def __init__(self, path):
        self.report = read_json(path)
        self.vector = []

    def hashing(self):
        h = FeatureHasher(n_features=188)
        hash_dict = dict()

        for i in self.report.items():
            hash_dict[i[0]] = i[1]
        digest = h.transform([hash_dict])
        return digest.toarray().tolist()[0]

    def process_report(self):
        # 전체 데이터 사용 (그런데 피처 해싱을 곁들인)
        self.vector += self.hashing()
        return self.vector


class EmberParser:
    '''
        예제에서 사용하지 않은 특징도 사용하여 벡터화 할 것을 권장
    '''

    def __init__(self, path):
        self.report = read_json(path)
        self.vector = []

    def get_histogram_info(self):
        histogram = np.array(self.report["histogram"])
        total = histogram.sum()
        vector = histogram / total
        return vector.tolist()

    def get_string_info(self):
        strings = self.report["strings"]

        hist_divisor = float(strings['printables']) if strings['printables'] > 0 else 1.0
        vector = [
            strings['numstrings'],
            strings['avlength'],
            strings['printables'],
            strings['entropy'],
            strings['paths'],
            strings['urls'],
            strings['registry'],
            strings['MZ']
        ]
        vector += (np.asarray(strings['printabledist']) / hist_divisor).tolist()
        return vector

    def get_general_file_info(self):
        general = self.report["general"]
        vector = [
            general['size'], general['vsize'], general['has_debug'], general['exports'], general['imports'],
            general['has_relocations'], general['has_resources'], general['has_signature'], general['has_tls'],
            general['symbols']
        ]
        return vector

    def get_coff_info(self):
        if self.report["header"]["coff"]["timestamp"] == 0:
            return [1]
        else:
            return [0]

    def get_data_directories_info(self):
        # except runtime header
        dd = self.report["datadirectories"]
        val1 = 0
        for i in dd:
            if i["name"] == "CLR_RUNTIME_HEADER":
                val1 += i["virtual_address"]
        return [val1 / max(1, len(dd))]

    def get_clr_runtime_header_info(self):
        dd = self.report["datadirectories"]
        val1 = 0
        val2 = 0
        for i in dd:
            if i["name"] == "CLR_RUNTIME_HEADER":
                val1 = i["virtual_address"]
                val2 = i["size"]
        return [val1, val2]

    def get_section_info(self):
        sections = self.report["section"]["sections"]
        h = FeatureHasher(n_features=24)
        feat_length = len(sections)
        numerical_val = [0, 0, 0]
        numerical_arg = ["size", "entropy", "vsize"]
        ret = []
        hash_props = dict()

        for i in sections:
            for j in range(3):
                numerical_val[j] += i[numerical_arg[j]]

            for j in i["props"]:
                if j not in hash_props:
                    hash_props[j] = 1
                else:
                    hash_props[j] += 1
            hash_props[i["name"]] = 1

        for i in range(3):
            numerical_val[i] /= max(1, feat_length)

        digest = h.transform([hash_props])
        return ret + digest.toarray().tolist()[0]

    def get_imports_title_info(self):
        feats = 27
        ret = np.zeros(feats)

        imports = self.report["imports"]
        h = FeatureHasher(n_features=feats, input_type="string")
        hash_list = []
        for i in imports.keys():
            hash_list.append(i)

        if len(hash_list) == 0:
            return ret.tolist()

        digest = h.transform(hash_list).toarray()
        for i in range(digest.shape[0]):
            ret = np.add(ret, digest[i])
        return ret.tolist()

    def process_report(self):
        vector = []
        vector += self.get_general_file_info()
        vector += self.get_histogram_info()
        vector += self.get_string_info()
        vector += self.get_clr_runtime_header_info()
        vector += self.get_coff_info()
        vector += self.get_section_info()
        vector += self.get_imports_title_info()
        '''
            특징 추가
        '''
        return vector


class PestudioParser:
    '''
        사용할 특징을 선택하여 벡터화 할 것을 권장
    '''

    def __init__(self, path):
        self.na = "n/a"
        self.report = read_json(path)
        self.vector = []
        self.severity_tags = parse_set("util_pestudio/severity_tag.txt")
        self.mal_api1 = parse_set("util_pestudio/API_used_in_Malware_1.txt")
        self.mal_api2 = parse_set("util_pestudio/API_used_in_Malware_2.txt")

    def get_indicator_vector(self):
        ret = [0, 0]

        indicators = self.report["image"]["indicators"]
        if indicators["@hint"] == self.na or indicators["@hint"] == '0':
            return [0, 0]

        for i in indicators["indicator"]:
            if i["@xml-id"] in self.severity_tags:
                ret[0] += 1
            if int(i["@severity"]) == 1:
                ret[1] += 1

        return ret

    def get_compiler_stamp_info(self):
        if "file-header" not in self.report["image"]:
            return [0]

        token = self.report["image"]["file-header"]["compiler-stamp"]
        if int(token.split(" ")[-1]) > 2018:
            return [1]
        else:
            return [0]

    def get_seh_info(self):
        if "optional-header" not in self.report["image"]:
            return [0]

        val = self.report["image"]["optional-header"]["structured-exception-handling"]
        if val == "true":
            return [1]
        else:
            return [0]

    def get_section_info(self):
        ret = [0, 0, 0]
        if "sections" not in self.report["image"]:
            return ret

        token = self.report["image"]["sections"]["section"]
        if isinstance(token, dict):
            if token["@name"] and token["@writable"] == 'x':
                ret[0] += 1
            if token["@name"] == ".rsrc":
                if token["@entropy"] != self.na:
                    ret[1] = float(token["@entropy"])
                if token["@file-ratio"] != self.na:
                    ret[2] = float(token["@file-ratio"])
        else:
            for i in token:
                if i["@name"] == ".text" and i["@writable"] == "x":
                    ret[0] += 1
                if i["@name"] == ".rsrc":
                    if i["@entropy"] != self.na:
                        ret[1] = float(i["@entropy"])

                    if i["@file-ratio"] != self.na:
                        ret[2] = float(i["@file-ratio"])

        return ret

    def count_libraries_blacklist(self):
        if "libraries" not in self.report["image"]:
            return [0]

        ret = 0

        token = self.report["image"]["libraries"]
        if token["@hint"] == self.na or "library" not in token:
            return [0]
        elif isinstance(token["library"], dict):
            if token["library"]["@blacklist"] == 'x':
                ret += 1
        else:
            for i in token["library"]:
                if i["@blacklist"] == "x":
                    ret += 1
        return [ret]

    def get_imports_vector(self):
        mal_set = self.mal_api1 | self.mal_api2
        # import_count = 0
        blacklist = 0
        malicious = 0

        if "imports" not in self.report["image"] or "import" not in self.report["image"]["imports"]:
            return [0, 0]

        token = self.report["image"]["imports"]["import"]

        if isinstance(token, dict):
            if token["@blacklist"] == 'x':
                blacklist = 1
            if token["@name"] in mal_set:
                malicious = 1
        else:
            for i in token:
                if i["@blacklist"] == 'x':
                    blacklist += 1

                if i["@name"] not in mal_set:
                    continue
                else:
                    malicious += 1

        return [malicious, blacklist]

    def get_tls_callback_info(self):
        if "tls-callbacks" not in self.report["image"]:
            return [0]

        upx_w = 1
        null_w = 0.5
        tls = self.report["image"]["tls-callbacks"]
        if tls == self.na or tls["@hint"] == self.na:
            return [0]

        val = int(tls["@hint"])
        ret = 0

        if val == 1:
            if tls["callback"] is None:
                return [null_w]

            token = tls["callback"]["@section"].lower()
            if token.startswith("upx"):
                ret += upx_w
            else:
                ret += 0.1
        else:
            for i in tls["callback"]:
                if i is None:
                    ret += null_w
                    continue

                token = i["@section"].lower()

                if token.startswith("upx"):
                    ret += upx_w
                else:
                    ret += 0.1

        return [ret]

    def get_certification_info(self):
        if "certificate" not in self.report["image"]:
            return [0]

        if self.report["image"]["certificate"] == self.na:
            return [0.5]
        if self.report["image"]["certificate"] == "expired":
            return [1]
        else:
            return [0]

    def get_overlay_vector(self):
        if "overlay" not in self.report["image"]:
            return [0, 0, 0, 0]

        no_overlay = 0
        hint_ratio = 0
        file_ratio = 0
        mal_token = 0

        if isinstance(self.report["image"]["overlay"], str):
            no_overlay = 1
            mal_token = 1
            return [no_overlay, 0, 0, mal_token]

        regex_fraction = re.compile(r'[1-9][0-9]*/[1-9][0-9]*')

        hint_str = self.report["image"]["overlay"]["@hint"]
        file_ratio = self.report["image"]["overlay"]["file-ratio"][:-1]
        file_ratio = float(file_ratio) / 100
        m = regex_fraction.search(hint_str)

        if m is None:
            txt = self.report["image"]["overlay"]["first-bytes-text"]
            if isinstance(txt, str):
                is_kisa = txt.find("K I S A") != -1
                if is_kisa and file_ratio < 0.03:
                    mal_token = 1
        else:
            val = m.group(0)
            val = val.split('/')
            hint_ratio = abs(int(val[0]) / int(val[1]))

        return [no_overlay, hint_ratio, file_ratio, mal_token]

    def process_report(self):
        ret = []
        ret += self.get_indicator_vector()
        ret += self.get_section_info()
        # ret += self.get_compiler_stamp_info()
        ret += self.get_seh_info()
        ret += self.count_libraries_blacklist()
        ret += self.get_imports_vector()
        # ret += self.get_tls_callback_info()
        # ret += self.get_certification_info()
        ret += self.get_overlay_vector()
        return ret


def get_train_path(petype: str):
    checker = set()
    checker.add("EMBER")
    checker.add("PEMINER")
    checker.add("PESTUDIO")
    if petype not in checker:
        print("Invalid path!")
        return

    return f"데이터/{petype}/학습데이터/"


def get_valid_path(petype: str):
    checker = set()
    checker.add("EMBER")
    checker.add("PEMINER")
    checker.add("PESTUDIO")
    if petype not in checker:
        print("Invalid path!")
        return

    return f"데이터/{petype}/검증데이터/"


def get_test_path(petype: str):
    checker = set()
    checker.add("EMBER")
    checker.add("PEMINER")
    checker.add("PESTUDIO")
    if petype not in checker:
        print("Invalid path!")
        return

    return f"데이터/{petype}/테스트데이터/"


def process_dataset(datatype: str, _pestudio=False):
    basepath = ""
    labels = None

    if datatype == "TRAIN":
        labels = read_label_csv("데이터/학습데이터_정답.csv")
        basepath = get_train_path("PEMINER")
    elif datatype == "VALID":
        labels = read_label_csv("데이터/검증데이터_정답.csv")
        basepath = get_valid_path("PEMINER")
    else:
        print("레이블이 없는 데이터입니다.")
        return dict()

    datadict = dict()

    for fname in glob.glob(basepath + '*'):
        # Erase ".json"
        key = fname[:-5].split('/')[-1]
        # Insert feature vector
        datadict[key] = copy.deepcopy(PeminerParser(fname).process_report())

    if datatype == "TRAIN":
        basepath = get_train_path("EMBER")
    else:
        basepath = get_valid_path("EMBER")

    for fname in glob.glob(basepath + '*'):
        key = fname[:-5].split('/')[-1]
        datadict[key] += copy.deepcopy(EmberParser(fname).process_report())

    if _pestudio:
        if datatype == "TRAIN":
            basepath = get_train_path("PESTUDIO")
        else:
            basepath = get_valid_path("PESTUDIO")

        for i in labels.keys():
            path = basepath + i + ".json"
            # print(path)
            if not os.path.isfile(path):
                del datadict[i]
            else:
                datadict[i] += copy.deepcopy(PestudioParser(path).process_report())

    ret = {
        "data": [],
        "target": np.array([], dtype=np.uint8)
    }
    for key in labels.keys():
        if key in datadict:
            ret["data"].append(datadict[key])
            ret["target"] = np.append(ret["target"], labels[key])
    ret["data"] = np.array(ret["data"])
    return ret


def combine_dataset(data1: dict, data2: dict):
    return {
        "data": np.append(data1["data"], data2["data"], axis=0),
        "target": np.append(data1["target"], data2["target"]),
    }


def write_dataset(path: str, data: dict):
    dt = copy.deepcopy(data)
    if isinstance(dt["data"], np.ndarray):
        dt["data"] = dt["data"].tolist()
    if isinstance(dt["target"], np.ndarray):
        dt["target"] = dt["target"].tolist()
    with open(path, 'w', encoding="utf-8") as f:
        f.write(json.dumps(dt))


def process_prediction_target(datatype: str):
    if datatype != "VALID" and datatype != "TEST" and datatype != "TRAIN":
        print("잘못된 인자입니다.")
        return

    labels = None
    path1 = ""
    path2 = ""
    path3 = ""

    if datatype == "VALID":
        labels = read_label_csv("데이터/검증데이터_정답.csv")
        path1 = get_valid_path("PEMINER")
        path2 = get_valid_path("EMBER")
        path3 = get_valid_path("PESTUDIO")
    elif datatype == "TEST":
        path1 = get_test_path("PEMINER")
        path2 = get_test_path("EMBER")
        path3 = get_test_path("PESTUDIO")
    else:
        labels = read_label_csv("데이터/테스트데이터_정답.csv")
        path1 = get_train_path("PEMINER")
        path2 = get_train_path("EMBER")
        path3 = get_train_path("PESTUDIO")

    ret = {
        'X_names': [],
        'X': [],
        'y': [],
        'size': []
    }

    datadict = dict()

    for fname in glob.glob(path1 + '*'):
        key = fname[:-5].split('/')[-1]
        datadict[key] = copy.deepcopy(PeminerParser(fname).process_report())
    for fname in glob.glob(path2 + '*'):
        key = fname[:-5].split('/')[-1]
        datadict[key] += copy.deepcopy(EmberParser(fname).process_report())
    for fname in glob.glob(path3 + '*'):
        key = fname[:-5].split('/')[-1]
        # print(key)
        datadict[key] += copy.deepcopy(PestudioParser(fname).process_report())

    vector_size = set()

    for item in datadict.items():
        ret["X_names"].append(item[0])
        ret['X'].append(item[1])
        vector_size.add(len(item[1]))
        if datatype != "TEST":
            ret['y'].append(labels[item[0]])

    ret['size'] = sorted(list(vector_size))

    return ret


def write_prediction_dict(path: str, _data: dict):
    with open(path, 'w', encoding="utf-8") as f:
        f.write(json.dumps(_data))


def read_prediction_dict(path: str):
    if not os.path.isfile(path):
        print("Invalid path!!")
        return None

    return json.loads(open(path, 'r', encoding="utf-8").read())


def write_compilation_data():
    print("Start the task")
    train_nopestudio = process_dataset("TRAIN", False)
    write_dataset("preprocessed/training_nopestudio.json", train_nopestudio)
    valid_nopestudio = process_dataset("VALID", False)
    write_dataset("preprocessed/validation_nopestudio.json", valid_nopestudio)
    combine_nopestudio = combine_dataset(train_nopestudio, valid_nopestudio)
    train_nopestudio = None
    valid_nopestudio = None
    write_dataset("preprocessed/dataset_nopestudio.json", combine_nopestudio)
    combine_nopestudio = None

    train_pestudio = process_dataset("TRAIN", True)
    write_dataset("preprocessed/training_pestudio.json", train_pestudio)
    valid_pestudio = process_dataset("TRAIN", True)
    write_dataset("preprocessed/validation_pestudio.json", valid_pestudio)
    combine_pestudio = combine_dataset(train_pestudio, valid_pestudio)
    train_pestudio = None
    valid_pestudio = None
    write_dataset("preprocessed/dataset_pestudio.json", combine_pestudio)
    combine_pestudio = None
    print("Task finished")


def read_dataset(path: str):
    if not os.path.isfile(path):
        print("Invalid path!!")
        return None

    txt = open(path, 'r', encoding="utf-8").read()
    ret = json.loads(txt)
    ret["data"] = np.array(ret["data"])
    ret["target"] = np.array(ret["target"])
    return ret


if __name__ == "__main__":
    write_compilation_data()
    # data = process_prediction_target("VALID")
    # write_prediction_dict("preprocessed/prediction_valid.json", data)
