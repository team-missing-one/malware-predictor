import numpy as np
import json
import glob
import copy
import os.path
from sklearn.feature_extraction import FeatureHasher


def read_label_csv(path):
    label_table = dict()
    with open(path, "r", encoding='ISO-8859-1') as f:
        for line in f.readlines()[1:]:
            fname, label = line.strip().split(",")
            label_table[fname] = int(label)
    return label_table


def read_json(path):
    with open(path, "r") as f:
        return json.load(f)


class PeminerParser:
    def __init__(self, path):
        self.report = read_json(path)
        self.vector = []

    def hashing(self):
        h = FeatureHasher(n_features=190)
        hash_dict = dict()

        for i in self.report.items():
            hash_dict[i[0]] = i[1]
        digest = h.transform([hash_dict])

        return digest.toarray().tolist()[0]

    def process_report(self):
        # 전체 데이터 사용 (그런데 피처 해싱을 곁들인)
        self.vector += self.hashing()
        return self.vector


class EmberParser:
    '''
        예제에서 사용하지 않은 특징도 사용하여 벡터화 할 것을 권장
    '''

    def __init__(self, path):
        self.report = read_json(path)
        self.vector = []

    def get_histogram_info(self):
        histogram = np.array(self.report["histogram"])
        total = histogram.sum()
        vector = histogram / total
        return vector.tolist()

    def get_string_info(self):
        strings = self.report["strings"]

        hist_divisor = float(strings['printables']) if strings['printables'] > 0 else 1.0
        vector = [
            strings['numstrings'],
            strings['avlength'],
            strings['printables'],
            strings['entropy'],
            strings['paths'],
            strings['urls'],
            strings['registry'],
            strings['MZ']
        ]
        vector += (np.asarray(strings['printabledist']) / hist_divisor).tolist()
        return vector

    def get_general_file_info(self):
        general = self.report["general"]
        vector = [
            general['size'], general['vsize'], general['has_debug'], general['exports'], general['imports'],
            general['has_relocations'], general['has_resources'], general['has_signature'], general['has_tls'],
            general['symbols']
        ]
        return vector

    def get_data_directories_info(self):
        # except runtime header
        dd = self.report["datadirectories"]
        val1 = 0
        for i in dd:
            if i["name"] == "CLR_RUNTIME_HEADER":
                val1 += i["virtual_address"]
        return [val1 / max(1, len(dd))]

    def get_clr_runtime_header_info(self):
        dd = self.report["datadirectories"]
        val1 = 0
        val2 = 0
        for i in dd:
            if i["name"] == "CLR_RUNTIME_HEADER":
                val1 = i["virtual_address"]
                val2 = i["size"]
        return [val1, val2]

    def get_section_info(self):
        sections = self.report["section"]["sections"]
        h = FeatureHasher(n_features=24)
        feat_length = len(sections)
        numerical_val = [0, 0, 0]
        numerical_arg = ["size", "entropy", "vsize"]
        ret = []
        hash_props = dict()

        for i in sections:
            for j in range(3):
                numerical_val[j] += i[numerical_arg[j]]

            for j in i["props"]:
                if j not in hash_props:
                    hash_props[j] = 1
                else:
                    hash_props[j] += 1
            hash_props[i["name"]] = 1

        for i in range(3):
            numerical_val[i] /= max(1, feat_length)

        digest = h.transform([hash_props])
        return ret + digest.toarray().tolist()[0]

    def get_imports_title_info(self):
        feats = 27
        ret = np.zeros(feats)

        imports = self.report["imports"]
        h = FeatureHasher(n_features=feats, input_type="string")
        hash_list = []
        for i in imports.keys():
            hash_list.append(i)

        if len(hash_list) == 0:
            return ret.tolist()

        digest = h.transform(hash_list).toarray()
        for i in range(digest.shape[0]):
            ret = np.add(ret, digest[i])
        return ret.tolist()

    def process_report(self):
        vector = []
        vector += self.get_general_file_info()
        vector += self.get_histogram_info()
        vector += self.get_string_info()
        vector += self.get_clr_runtime_header_info()
        vector += self.get_section_info()
        vector += self.get_imports_title_info()
        '''
            특징 추가
        '''
        return vector


class PestudioParser:
    '''
        사용할 특징을 선택하여 벡터화 할 것을 권장
    '''

    def __init__(self, path):
        self.report = read_json(path)
        self.vector = []

    def process_report(self):
        ret = []

        vir = self.report["image"]["virustotal"]
        if vir != "offline" and vir != "n/a":
            ret.append(int(vir["@detection"]))
        else:
            ret.append(0)
        
        return ret


def get_train_path(petype: str):
    checker = set()
    checker.add("EMBER")
    checker.add("PEMINER")
    checker.add("PESTUDIO")
    if petype not in checker:
        print("Invalid path!")
        return

    return f"데이터/{petype}/학습데이터/"


def get_valid_path(petype: str):
    checker = set()
    checker.add("EMBER")
    checker.add("PEMINER")
    checker.add("PESTUDIO")
    if petype not in checker:
        print("Invalid path!")
        return

    return f"데이터/{petype}/검증데이터/"


def get_test_path(petype: str):
    checker = set()
    checker.add("EMBER")
    checker.add("PEMINER")
    checker.add("PESTUDIO")
    if petype not in checker:
        print("Invalid path!")
        return

    return f"데이터/{petype}/테스트데이터/"


def process_dataset(datatype: str):
    basepath = ""
    labels = None

    if datatype == "TRAIN":
        labels = read_label_csv("데이터/학습데이터_정답.csv")
        basepath = get_train_path("PEMINER")
    elif datatype == "VALID":
        labels = read_label_csv("데이터/검증데이터_정답.csv")
        basepath = get_valid_path("PEMINER")
    else:
        print("레이블이 없는 데이터입니다.")
        return dict()

    datadict = dict()

    for fname in glob.glob(basepath + '*'):
        # Erase ".json"
        key = fname[:-5].split('/')[-1]
        # Insert feature vector
        datadict[key] = copy.deepcopy(PeminerParser(fname).process_report())

    if datatype == "TRAIN":
        basepath = get_train_path("EMBER")
    else:
        basepath = get_valid_path("EMBER")

    for fname in glob.glob(basepath + '*'):
        key = fname[:-5].split('/')[-1]
        datadict[key] += copy.deepcopy(EmberParser(fname).process_report())

    # TODO: Handle the pestudio
    if datatype == "TRAIN":
        basepath = get_train_path("PESTUDIO")
    else:
        basepath = get_valid_path("PESTUDIO")

    ret = {
        "data_names": [],
        "data": [],
        "target": np.array([], dtype=np.uint8)
    }
    for key in labels.keys():
        ret["data_names"].append(key)
        ret["data"].append(datadict[key])
        ret["target"] = np.append(ret["target"], labels[key])
    ret["data"] = np.array(ret["data"])
    return ret


def combine_dataset(data1: dict, data2: dict):
    return {
        "data": np.append(data1["data"], data2["data"], axis=0),
        "target": np.append(data1["target"], data2["target"]),
        "data_names": data1["data_names"] + data2["data_names"]
    }


def write_dataset(path: str, data: dict):
    dt = copy.deepcopy(data)
    dt["data"] = dt["data"].tolist()
    dt["target"] = dt["target"].tolist()
    with open(path, 'w', encoding="utf-8") as f:
        f.write(json.dumps(dt))


def read_dataset(path: str):
    if not os.path.isfile(path):
        print("Invalid path!!")
        return None

    txt = open(path, 'r', encoding="utf-8").read()
    ret = json.loads(txt)
    ret["data"] = np.array(ret["data"])
    ret["target"] = np.array(ret["target"])
    return ret
