{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Grid search 란 ?**"
      ],
      "metadata": {
        "id": "Lt9Da0EPU9c6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 단순 정의를 하자면 모델에게 가장 적합한 하이퍼 파라미터를 찾기 라고 할 수있다.\n",
        "Grid search (격자 탐색) 은 모델 하이퍼 파라미터에 넣을 수 있는 값들을\n",
        "\n",
        "순차적으로 입력한뒤에 가장 높은 성능을 보이는 하이퍼 파라미터들을 찾는 탐색 방법이다. \n",
        "\n",
        "모델을 학생에 비유해보자면, 학생에게 문제집을 공부시키기 위해선 여러가지 공부법이 있다.\n",
        "\n",
        "이 공부법들은 머신러닝/딥러닝 수많은 학습 모델 종류들에 해당 된다. \n",
        "\n",
        "하루에 몇쪽 풀고 모의고사 칠지, 몇번씩 보고 다음페이지 넘어갈지 어디까지 깊게 공부할지 등등\n",
        "\n",
        "이 세부적인 규율이 하이퍼 파라미터라 생각을 한다. 그리고 그리드 서치는 그 세부적인 규율을 일일히 다 적용해봐가면서\n",
        "\n",
        "어떤 규율이 가장 이 공부법(모델)에 적합한지 판단하는 것이다."
      ],
      "metadata": {
        "id": "92HA4LVMU_nx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 그렇다면 하이퍼파라미터란?\n",
        "\n",
        " 모델을 생성할 때, 사용자가 직접 설정하는 변수가 사전적 정의이다. \n",
        "\n",
        "랜덤 포레스트 모델을 우리가 만든다고 하면, 트리의 개수를 몇개까지 할 것인지. 트리의 깊이는 몇까지 할 것인지\n",
        "\n",
        "딥러닝 모델에서는 layer의 개수, 에폭(학습횟수)의 수 등이 된다.\n",
        "\n",
        " \n",
        "\n",
        "즉, 그 세부적인 규율은 모델을 학습시키는 우리가 정할 수 있다는 것이다."
      ],
      "metadata": {
        "id": "xSXEkUARVatK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **파라미터와 하이퍼 파라미터의 차이 **\n",
        "\n",
        "반면 파라미터(parameter, 매개변수)는 학습 과정에서 생성되는 변수들이다. \n",
        "\n",
        "랜덤 포레스트 모델과 딥러닝 모델의 학습 가중치 들이 파라미터에 해당된다. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XQegBvhHVqPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 그리드 서치를 하는 이유는\n",
        "\n",
        "# **\" 가장 우수한 성능을 보이는 모델의 하이퍼 파라미터를 찾기 위해서 \"**\n",
        "\n",
        "모든 경우의 수를 넣어 보고 가장 성능이 좋게 만드는 모델의 하이퍼 파라미터를 찾는것이다."
      ],
      "metadata": {
        "id": "Yfv4pPStV8qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **적용방법**\n",
        "GridSearchCV(estimator,param_grid,scoring,cv,refit) 함수형태를 가지고 있다.\n",
        "\n",
        "각각의 파라미터는 이러한 기능을 한다.\n",
        "\n",
        "\n",
        "-estimator : classifier, regressor, pipeline 등 가능\n",
        "\n",
        "-param_grid : 튜닝을 위해 파라미터, 사용될 파라미터를 dictionary 형태로 만들어서 넣는다.\n",
        "\n",
        "-scoring : 예측 성능을 측정할 평가 방법을 넣는다. 보통 accuracy 로 지정하여서 정확도로 성능 평가를 한다.\n",
        "\n",
        "-cv : 교차 검증에서 몇개로 분할되는지 지정한다.\n",
        "\n",
        "-refit : True가 디폴트로 True로 하면 최적의 하이퍼 파라미터를 찾아서 재학습 시킨다.\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "sntXdgv2eXFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **간단한 예제**"
      ],
      "metadata": {
        "id": "VBbTvp4zfbvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding:utf-8 -*-\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import json\n",
        "\n",
        "# 회귀용 샘플 데이터 불러오기\n",
        "iris = load_iris()\n",
        "X, y,labels = iris.data, iris.target,iris.target_names\n",
        "\n",
        "X, y,labels = data[\"data\"], data[\"target\"],data[\"data_names\"]\n",
        "\n",
        "# 학습/테스트 데이터셋 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)\n",
        "\n",
        "# 랜덤포레스트 + 그리드서치로 모델 학습\n",
        "rf = RandomForestRegressor(random_state=1)\n",
        "param_grid = [{'n_estimators': range(5, 50, 10), 'max_features': range(1, 4), 'max_depth': range(3, 5)}]\n",
        "gs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='r2', cv=5, n_jobs=-1)\n",
        "gs.fit(X_train, y_train)\n",
        "\n",
        "# 그리드서치 학습 결과 출력\n",
        "print('베스트 하이퍼 파라미터: {0}'.format(gs.best_params_))\n",
        "print('베스트 하이퍼 파라미터 일 때 R^2 점수: {0:.2f}'.format(gs.best_score_))\n",
        "\n",
        "# 최적화 모델 추출\n",
        "model = gs.best_estimator_\n",
        "\n",
        "# 테스트세트 R^2 점수 출력\n",
        "r2_score = model.score(X_test, y_test)\n",
        "print('테스트세트에서의 R^2 점수: {0:.2f}'.format(r2_score))\n",
        "\n",
        "# 테스트세트 예측 결과 샘플 출력\n",
        "predicted_y = model.predict(X_test)\n",
        "for i in range(10):\n",
        "    print('실제 값: {0:.2f}, 예측 값: {1:.2f}'.format(y_test[i], predicted_y[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "eZZ8NX3Ogs8i",
        "outputId": "d06b53fd-dac0-4212-fcf5-b476f52eb6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ee181b46327a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pwd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 122683392 (char 122683391)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding:utf-8 -*-\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# 회귀용 샘플 데이터 불러오기\n",
        "iris = load_iris()\n",
        "X, y,labels = iris.data, iris.target,iris.target_names\n",
        "\n",
        "# 학습/테스트 데이터셋 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)\n",
        "\n",
        "# LGBM + 그리드서치로 모델 학습\n",
        "rf = RandomForestRegressor(random_state=1)\n",
        "lgbm = LGBMClassifier(n_estimators=200)\n",
        "params_lgbm = {'max_depth': [10, 15, 20],'min_child_samples': [20, 40, 60],'subsample': [0.8, 1]}\n",
        "gs = GridSearchCV(lgbm, param_grid=params_lgbm)\n",
        "gs.fit(X_train, y_train)\n",
        "\n",
        "# 그리드서치 학습 결과 출력\n",
        "print('베스트 하이퍼 파라미터: {0}'.format(gs.best_params_))\n",
        "print('베스트 하이퍼 파라미터 일 때 R^2 점수: {0:.2f}'.format(gs.best_score_))\n",
        "\n",
        "# 최적화 모델 추출\n",
        "model = gs.best_estimator_\n",
        "\n",
        "# 테스트세트 R^2 점수 출력\n",
        "r2_score = model.score(X_test, y_test)\n",
        "print('테스트세트에서의 R^2 점수: {0:.2f}'.format(r2_score))\n",
        "\n",
        "# 테스트세트 예측 결과 샘플 출력\n",
        "predicted_y = model.predict(X_test)\n",
        "for i in range(10):\n",
        "    print('실제 값: {0:.2f}, 예측 값: {1:.2f}'.format(y_test[i], predicted_y[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmFwalPQioCz",
        "outputId": "fa4d6cd1-c3b3-4d98-fd9f-c97136355cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "베스트 하이퍼 파라미터: {'max_depth': 10, 'min_child_samples': 20, 'subsample': 0.8}\n",
            "베스트 하이퍼 파라미터 일 때 R^2 점수: 0.92\n",
            "테스트세트에서의 R^2 점수: 0.98\n",
            "실제 값: 2.00, 예측 값: 2.00\n",
            "실제 값: 0.00, 예측 값: 0.00\n",
            "실제 값: 0.00, 예측 값: 0.00\n",
            "실제 값: 2.00, 예측 값: 1.00\n",
            "실제 값: 1.00, 예측 값: 1.00\n",
            "실제 값: 1.00, 예측 값: 1.00\n",
            "실제 값: 2.00, 예측 값: 2.00\n",
            "실제 값: 1.00, 예측 값: 1.00\n",
            "실제 값: 2.00, 예측 값: 2.00\n",
            "실제 값: 0.00, 예측 값: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "\n",
        "def load_model(**kwargs):\n",
        "    if kwargs[\"model\"] == \"rf\":\n",
        "\n",
        "        return RandomForestClassifier(\n",
        "            random_state=kwargs[\"random_state\"],\n",
        "            n_jobs=4,\n",
        "            criterion=\"entropy\",\n",
        "            n_estimators=500\n",
        "        )\n",
        "    elif kwargs[\"model\"] == \"rfgrid\":\n",
        "        rf= RandomForestClassifier(\n",
        "            random_state=kwargs[\"random_state\"],\n",
        "            n_jobs=4,\n",
        "            criterion=\"entropy\",\n",
        "            n_estimators=500\n",
        "        )\n",
        "        param_grid = [{'n_estimators': range(5, 50, 10), 'max_features': range(1, 4), 'max_depth': range(3, 5)}]\n",
        "        gs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=1)\n",
        "\n",
        "        return gs\n",
        "    elif kwargs[\"model\"] == \"dt\":\n",
        "        return DecisionTreeClassifier(\n",
        "            random_state=kwargs[\"random_state\"],\n",
        "            criterion=\"entropy\",\n",
        "            max_depth=8\n",
        "        )\n",
        "    elif kwargs[\"model\"] == \"lgb\":\n",
        "        return LGBMClassifier(\n",
        "            random_state=kwargs[\"random_state\"],\n",
        "            objective=\"binary\",\n",
        "            metric=\"binary_logloss\",\n",
        "            class_weight=\"balanced\",\n",
        "            max_depth=30,\n",
        "            num_leaves=1200\n",
        "        )\n",
        "    elif kwargs[\"model\"] == \"lgbgrid\":\n",
        "        lgbm = LGBMClassifier(\n",
        "            random_state=kwargs[\"random_state\"],\n",
        "            objective=\"binary\",\n",
        "            metric=\"binary_logloss\",\n",
        "            class_weight=\"balanced\",\n",
        "            max_depth=30,\n",
        "            num_leaves=1200\n",
        "        )\n",
        "        params_lgbm = {'max_depth': [10, 15, 20], 'min_child_samples': [20, 40, 60], 'subsample': [0.8, 1]}\n",
        "        gs = GridSearchCV(estimator=lgbm, param_grid=params_lgbm)\n",
        "        return gs\n",
        "\n",
        "    elif kwargs[\"model\"] == \"svm\":\n",
        "        return SVC(random_state=kwargs[\"random_state\"])\n",
        "    elif kwargs[\"model\"] == \"lr\":\n",
        "        return LogisticRegression(random_state=kwargs[\"random_state\"], n_jobs=-1)\n",
        "    elif kwargs[\"model\"] == \"knn\":\n",
        "        return KNeighborsClassifier(n_jobs=-1)\n",
        "    elif kwargs[\"model\"] == \"adacat\":\n",
        "        bm = CatBoostClassifier(\n",
        "            random_seed=kwargs[\"random_state\"],\n",
        "            logging_level=\"Silent\",\n",
        "            loss_function=\"Logloss\",\n",
        "            eval_metric=\"AUC\",\n",
        "            class_weights=(2.1, 1),\n",
        "            thread_count=-1,\n",
        "            early_stopping_rounds=10\n",
        "        )\n",
        "        return AdaBoostClassifier(\n",
        "            base_estimator=bm,\n",
        "            n_estimators=376,\n",
        "            random_state=kwargs[\"random_state\"]\n",
        "        )\n",
        "    elif kwargs[\"model\"] == \"adalgbm\":\n",
        "        bm = LGBMClassifier(\n",
        "            random_state=kwargs[\"random_state\"],\n",
        "            objective=\"binary\",\n",
        "            metric=\"binary_logloss\",\n",
        "            class_weight=\"balanced\",\n",
        "            max_depth=30,\n",
        "            num_leaves=1200\n",
        "        )\n",
        "        return AdaBoostClassifier(\n",
        "            base_estimator=bm,\n",
        "            n_estimators=376,\n",
        "            random_state=kwargs[\"random_state\"]\n",
        "        )\n",
        "    elif kwargs[\"model\"] == \"mlp\":\n",
        "        return MLPClassifier(\n",
        "            random_state=kwargs[\"random_state\"],\n",
        "            activation=\"relu\",\n",
        "            solver=\"adam\",\n",
        "            batch_size=128,\n",
        "            learning_rate_init=0.01,\n",
        "            hidden_layer_sizes=(20, 12)\n",
        "        )\n",
        "    elif kwargs[\"model\"] == \"cat\":\n",
        "        return CatBoostClassifier(\n",
        "            random_seed=kwargs[\"random_state\"],\n",
        "            logging_level=\"Silent\",\n",
        "            loss_function=\"Logloss\",\n",
        "            eval_metric=\"AUC\",\n",
        "            class_weights=(2.1, 1),\n",
        "            thread_count=-1,\n",
        "            early_stopping_rounds=10\n",
        "        )\n",
        "    else:\n",
        "        print(\"Unsupported Algorithm\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def train(X_train, y_train, model):\n",
        "    '''\n",
        "        머신러닝 모델을 선택하여 학습을 진행하는 함수\n",
        "        :param X_train: 학습할 2차원 리스트 특징벡터\n",
        "        :param y_train: 학습할 1차원 리스트 레이블 벡터\n",
        "        :param model: 문자열, 선택할 머신러닝 알고리즘\n",
        "        :return: 학습된 머신러닝 모델 객체\n",
        "    '''\n",
        "    clf = load_model(model=model, random_state=SEED)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    return clf\n",
        "\n",
        "\n",
        "def evaluate(X_test, y_test, model):\n",
        "    '''\n",
        "        학습된 머신러닝 모델로 검증 데이터를 검증하는 함수\n",
        "        :param X_test: 검증할 2차원 리스트 특징 벡터\n",
        "        :param y_test: 검증할 1차원 리스트 레이블 벡터\n",
        "        :param model: 학습된 머신러닝 모델 객체\n",
        "    '''\n",
        "    predict = model.predict(X_test)\n",
        "    print(\"정확도\", model.score(X_test, y_test))\n",
        "\n",
        "\n",
        "def evaluate_kfold(X_train, y_train, model):\n",
        "    kfold = StratifiedKFold(n_splits=10).split(X_train, y_train)\n",
        "    scores = []\n",
        "    for k, (_train, _test) in enumerate(kfold):\n",
        "        model.fit(X_train[_train], y_train[_train])\n",
        "\n",
        "        local_prediction = model.predict(X_train[_test])\n",
        "        score_acc = accuracy_score(local_prediction, y_train[_test])\n",
        "        score_f1 = f1_score(local_prediction, y_train[_test])\n",
        "        scores.append(score_acc)\n",
        "        cf = confusion_matrix(local_prediction, y_train[_test])\n",
        "        print(f\"폴드: {k + 1}, 클래스 분포: {np.bincount(y_train[_test])}, 정확도: {score_acc:.4f}, f1: {score_f1:.4f}\")\n",
        "        print(cf)\n",
        "    print(\"CV 정확도: %.4f +/- %.4f\" % (np.mean(scores), np.std(scores)))\n",
        "\n",
        "    # scores = cross_validate(\n",
        "    #     estimator=model,\n",
        "    #     X=X_train,\n",
        "    #     y=y_train,\n",
        "    #     scoring=[\"accuracy\", \"f1\"],\n",
        "    #     cv=10,\n",
        "    #     n_jobs=-1,\n",
        "    #     return_train_score=False\n",
        "    # )\n",
        "\n",
        "    # scores = cross_val_score(\n",
        "    #     estimator=model,\n",
        "    #     X=X_train,\n",
        "    #     y=y_train,\n",
        "    #     cv=10,\n",
        "    #     n_jobs=1\n",
        "    # )\n",
        "    # print(\"CV 정확도 점수: %s\" % scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "fOry2hONYlsi",
        "outputId": "c208ca12-0027-46c3-b842-1a496d647c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-5f7bc90ba5ec>\"\u001b[0;36m, line \u001b[0;32m35\u001b[0m\n\u001b[0;31m    return f\"데이터\\{petype}\\학습데이터\\\"\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QlhOS5yJYoQK"
      }
    }
  ]
}