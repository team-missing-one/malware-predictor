{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "report.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **AI 프로젝트**\n",
        "정보보호와 시스템보안 <br>\n",
        " <br>\n",
        "\n",
        "### **Missing One 팀원**\n",
        "\n",
        "\n",
        "*   백승렬(20202086)\n",
        "*   이윤서(20181676)\n",
        "*   최호경(20171717)\n",
        "*   한진교(20152868)\n",
        "\n",
        " <br>\n",
        "\n",
        "\n",
        "\n",
        "### **이번 프로젝트에서의 핵심**\n",
        "\n",
        "1.   기존의 lightGBM 모델 외에도 Categorial을 다루는 catBoost모델을 도입하여 악성코드 탐지를 수행하는 인공지능의 성능 향상\n",
        "2.   k-fold 교차 검증법 구현\n",
        "3.   시그니처를 기반으로 탐지를 실행하여 인공지능 모델의 정확도를 높임\n",
        "4.   데이터 리포트 파일을 분석 후, 악성 파일과 정상 파일의 차이점이 뚜렷하지 않아 API처리로 접근함 -> 악성 API 리스트를 텍스트 파일로 정리한 뒤, 벡터로 표현하여 학습시키면 모델이 API 가중치를 반영함\n",
        "\n",
        " <br>\n",
        "\n",
        "### **악성코드에 쓰이는 API 수집 목록**\n",
        "read <br>\n",
        "recv <br>\n",
        "WSACleanup <br>\n",
        "RegCreateKeyEx  <br>\n",
        "GetTempPath <br>\n",
        "OpenSCManager <br>\n",
        "RegOpenKeyEx <br>\n",
        "CopyFile <br>\n",
        "CreateService <br>\n",
        "RegSetValueEx <br>\n",
        "CreateFile <br>\n",
        "StartServiceCtrlDispatcher <br>\n",
        "RegDeleteKeyEx <br>\n",
        "WriteFile <br>\n",
        "RegGetValue <br>\n",
        "ReadFile <br>\n",
        "CryptAcquireContext <br>\n",
        "CryptGenKey <br>\n",
        "CryptDeriveKey <br>\n",
        "CryptDecrypt <br>\n",
        "CryptReleaseContext <br>\n",
        "IsDebuggerPresent <br>\n",
        "GetSystemInfo <br>\n",
        "GlobalMemoryStatusEx <br>\n",
        "GetVersion <br>\n",
        "VirtualAlloc <br>\n",
        "VirtualProtect <br>\n",
        "ReadProcessMemory <br>\n",
        "WriteProcessMemory <br>\n",
        "NtWriteVirtualMemory <br>\n",
        "CreateRemoteThread <br>\n",
        "NtUnmapViewOfSection <br>\n",
        "QueueUserAPC <br>\n",
        "CreateProcessInternal  <br>\n",
        "CreateProcess <br>\n",
        "ShellExecute <br>\n",
        "WinExec <br>\n",
        "ResumeThread  <br>\n",
        "NtResumeThread  <br>\n",
        "GetAsyncState()  <br>\n",
        "SetWindowsHookEx  <br>\n",
        "GetForeGroundWindow  <br>\n",
        "LoadLibrary <br>\n",
        "GetProcAddress <br>\n",
        "CreateToolhelp32Snapshot  <br>\n",
        "GetDC <br>\n",
        "BitBlt <br>\n",
        "internetOpen  <br>\n",
        "internetOpenUrl  <br>\n",
        "InternetReadFile  <br>\n",
        "InternetWriteFile <br>\n",
        "FindResource <br>\n",
        "LoadResource <br>\n",
        "LockResource <br>\n",
        "Accept <br>\n",
        "AdjustTokenPrivileges <br>\n",
        "AttachThreadInput <br>\n",
        "Bind <br>\n",
        "BitBlt <br>\n",
        "CertOpenSystemStore <br>\n",
        "Connect <br>\n",
        "ConnectNamedpipe <br>\n",
        "ControlService <br>\n",
        "CreateFile <br>\n",
        "CreateFileMapping <br>\n",
        "CreateMutex <br>\n",
        "CreateProcess <br>\n",
        "CreateRemoteThread <br>\n",
        "CreateService <br>\n",
        "CreateToolhelp32Snapshot <br>\n",
        "CryptAcuireContext <br>\n",
        "DeviceIoControl <br>\n",
        "EnableExecuteProtectionSupport <br>\n",
        "EnumProcesses <br>\n",
        "EnumProcessModules <br>\n",
        "FindFirstFile <br>\n",
        "FindNextFile <br>\n",
        "FindResource <br>\n",
        "FindWindow <br>\n",
        "FtpPutFile <br>\n",
        "GetAdapterInfo <br>\n",
        "GetAsyncKeyState <br>\n",
        "GetDC <br>\n",
        "GetForegroundWindow <br>\n",
        "Gethostbyname <br>\n",
        "Gethostname <br>\n",
        "GetModuleFilename <br>\n",
        "GetModuleHandle <br>\n",
        "GetProcAddress <br>\n",
        "GetStartupInfo <br>\n",
        "GetSystemDefaultLangId <br>\n",
        "GetTempPath <br>\n",
        "GetThreadContext <br>\n",
        "GetVersionEx <br>\n",
        "GetWindowsDirectory <br>\n",
        "inet_addr <br>\n",
        "InternetOpen <br>\n",
        "InternetOpenUrl <br>\n",
        "InternetReadFile <br>\n",
        "InternetWriteFile <br>\n",
        "IsNTAdmin <br>\n",
        "IsWoW64Process <br>\n",
        "LdrLoadDll <br>\n",
        "LoadResource <br>\n",
        "LsaEnumerateLogonSessions <br>\n",
        "MapViewOfFile <br>\n",
        "MapVirtualKey <br>\n",
        "Module32First/Module32Next <br>\n",
        "NetScheduleJobAdd <br>\n",
        "NetShareEnum <br>\n",
        "NtQueryDirectoryFile <br>\n",
        "NtQueryInformationProcess <br>\n",
        "NtSetInformationProcess <br>\n",
        "OpenMutex <br>\n",
        "OpenProcess <br>\n",
        "OutputDebugString <br>\n",
        "PeekNamedPipe <br>\n",
        "Process32First <br>\n",
        "Process32Next <br>\n",
        "QueueUserAPC <br>\n",
        "ReadProcessMemory <br>\n",
        "Recv <br>\n",
        "RegisterHotKey <br>\n",
        "RegOpenKey <br>\n",
        "ResumeThread <br>\n",
        "RtlCreateRegistryKey <br>\n",
        "RtlWriteRegistryValue <br>\n",
        "SamIConnect <br>\n",
        "SamIGetPrivateData <br>\n",
        "SamQueryInformationUse <br>\n",
        "Send <br>\n",
        "SetFileTime <br>\n",
        "SetThreadContext <br>\n",
        "SetWindowsHookEx <br>\n",
        "SfcTerminateWatcherThread <br>\n",
        "ShellExecute <br>\n",
        "StartServiceCtrlDispatcher <br>\n",
        "SuspendThread <br>\n",
        "System <br>\n",
        "Thread32First <br>\n",
        "Thread32Next <br>\n",
        "Toolhelp32ReadProcessMemory <br>\n",
        "URLDownloadToFile <br>\n",
        "VirtualAllocEx <br>\n",
        "VirtualProtectEx <br>\n",
        "WideCharToMultiByte <br>\n",
        "WinExec <br>\n",
        "WriteProcessMemory <br>\n",
        "WSAStartup <br>\n",
        "\n",
        " <br>\n",
        " \n",
        "### **각 모델별 정확도 계산**\n",
        "\n",
        "*   rf : 0.944\n",
        "*   lgb : 0.9657\n",
        "*   항목 추가\n",
        "*   항목 추가\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "STVlAP4WyqN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3n_E4xQZKXgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Jgs_QenMIf2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **model_lib.py**\n",
        "\n",
        "1.   Seed 값을 42에서 기존 스켈레톤 코드의 41로 조정하였습니다.\n",
        "2.   소프트 보팅(Soft Voting) 알고리즘을 추가하였으며, adaboost를 사용하는 모델의 n_estimator 파라미터를 변경하였습니다.\n",
        "3. 홀드아웃 검증을 할 수 있도록 변경하였습니다. \n",
        "4. load model 함수에서 어떤 모델들이 있는지 볼 수 있고, 앙상블 알고리즘을 넣을 수 있도록 했습니다.\n",
        "5. 스케일러를 추가로 임포트 하였습니다.\n"
      ],
      "metadata": {
        "id": "BJxaKJJgISP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "\n",
        "SEED = 41\n",
        "\n",
        "\n",
        "def load_model(**kwargs):\n",
        "    if kwargs[\"model\"] == \"rf\":\n",
        "        return RandomForestClassifier(\n",
        "            random_state=kwargs[\"random_state\"],\n",
        "            n_jobs=4,\n",
        "            criterion=\"entropy\",\n",
        "            n_estimators=500\n",
        "        )\n",
        "    elif kwargs[\"model\"] == \"dt\":\n",
        "        return DecisionTreeClassifier(\n",
        "            random_state=kwargs[\"random_state\"],\n",
        "            criterion=\"entropy\",\n",
        "            max_depth=8\n",
        "        )\n",
        "    elif kwargs[\"model\"] == \"lgb\":\n",
        "        return LGBMClassifier(\n",
        "            random_state=kwargs[\"random_state\"],\n",
        "            objective=\"binary\",\n",
        "            metric=\"binary_logloss\",\n",
        "            class_weight=\"balanced\",\n",
        "            max_depth=30,\n",
        "            num_leaves=1200\n",
        "        )\n",
        "    elif kwargs[\"model\"] == \"svm\":\n",
        "        return SVC(random_state=kwargs[\"random_state\"])\n",
        "    elif kwargs[\"model\"] == \"lr\":\n",
        "        return LogisticRegression(random_state=kwargs[\"random_state\"], n_jobs=-1)\n",
        "    elif kwargs[\"model\"] == \"knn\":\n",
        "        return KNeighborsClassifier(n_jobs=-1)\n",
        "    elif kwargs[\"model\"] == \"adacat\":\n",
        "        bm = CatBoostClassifier(\n",
        "            random_seed=kwargs[\"random_state\"],\n",
        "            logging_level=\"Silent\",\n",
        "            loss_function=\"Logloss\",\n",
        "            eval_metric=\"AUC\",\n",
        "            class_weights=(2.1, 1),\n",
        "            thread_count=-1,\n",
        "            early_stopping_rounds=10\n",
        "        )\n",
        "        return AdaBoostClassifier(\n",
        "            base_estimator=bm,\n",
        "            n_estimators=100,\n",
        "            random_state=kwargs[\"random_state\"]\n",
        "        )\n",
        "    elif kwargs[\"model\"] == \"adalgbm\":\n",
        "        bm = LGBMClassifier(\n",
        "            random_state=kwargs[\"random_state\"],\n",
        "            objective=\"binary\",\n",
        "            metric=\"binary_logloss\",\n",
        "            class_weight=\"balanced\",\n",
        "            max_depth=30,\n",
        "            num_leaves=1200\n",
        "        )\n",
        "        return AdaBoostClassifier(\n",
        "            base_estimator=bm,\n",
        "            n_estimators=100,\n",
        "            random_state=kwargs[\"random_state\"]\n",
        "        )\n",
        "    elif kwargs[\"model\"] == \"mlp\":\n",
        "        return MLPClassifier(\n",
        "            random_state=kwargs[\"random_state\"],\n",
        "            activation=\"relu\",\n",
        "            solver=\"adam\",\n",
        "            batch_size=128,\n",
        "            learning_rate_init=0.01,\n",
        "            hidden_layer_sizes=(20, 12)\n",
        "        )\n",
        "    elif kwargs[\"model\"] == \"cat\":\n",
        "        return CatBoostClassifier(\n",
        "            random_seed=kwargs[\"random_state\"],\n",
        "            logging_level=\"Silent\",\n",
        "            loss_function=\"Logloss\",\n",
        "            eval_metric=\"AUC\",\n",
        "            class_weights=(2.1, 1),\n",
        "            thread_count=-1,\n",
        "            early_stopping_rounds=10\n",
        "        )\n",
        "    elif kwargs[\"model\"] == \"voting\":\n",
        "        rf = load_model(model=\"rf\", random_state=kwargs[\"random_state\"])\n",
        "        lgbm = load_model(model=\"lgb\", random_state=kwargs[\"random_state\"])\n",
        "        cat = load_model(model=\"cat\", random_state=kwargs[\"random_state\"])\n",
        "        adalgbm = load_model(model=\"adalgbm\", random_state=kwargs[\"random_state\"])\n",
        "        adacat = load_model(model=\"adacat\", random_state=kwargs[\"random_state\"])\n",
        "        voters = [\n",
        "            (\"rf\", rf),\n",
        "            (\"lgbm\", lgbm),\n",
        "            (\"catboost\", cat),\n",
        "            (\"lgbmXadaboost\", adalgbm),\n",
        "            (\"catboostXadaboost\", adacat),\n",
        "        ]\n",
        "        w = [1, 1.8, 1.6, 1.9, 2]\n",
        "        return VotingClassifier(estimators=voters, voting=\"soft\")\n",
        "    else:\n",
        "        print(\"Unsupported Algorithm\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def train(X_train, y_train, model):\n",
        "    '''\n",
        "        머신러닝 모델을 선택하여 학습을 진행하는 함수\n",
        "        :param X_train: 학습할 2차원 리스트 특징벡터\n",
        "        :param y_train: 학습할 1차원 리스트 레이블 벡터\n",
        "        :param model: 문자열, 선택할 머신러닝 알고리즘\n",
        "        :return: 학습된 머신러닝 모델 객체\n",
        "    '''\n",
        "    clf = load_model(model=model, random_state=SEED)\n",
        "    clf.fit(X_train, y_train)\n",
        "    # pipe = make_pipeline(\n",
        "    #     MinMaxScaler(),\n",
        "    #     clf\n",
        "    # )\n",
        "    # pipe.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "\n",
        "def evaluate(X_test, y_test, model):\n",
        "    '''\n",
        "        학습된 머신러닝 모델로 검증 데이터를 검증하는 함수\n",
        "        :param X_test: 검증할 2차원 리스트 특징 벡터\n",
        "        :param y_test: 검증할 1차원 리스트 레이블 벡터\n",
        "        :param model: 학습된 머신러닝 모델 객체\n",
        "    '''\n",
        "    predict = model.predict(X_test)\n",
        "    print(\"정확도\", model.score(X_test, y_test))\n",
        "\n",
        "\n",
        "def evaluate_kfold(X_train, y_train, model):\n",
        "    kfold = StratifiedKFold(n_splits=10).split(X_train, y_train)\n",
        "    scores = []\n",
        "    for k, (_train, _test) in enumerate(kfold):\n",
        "        model.fit(X_train[_train], y_train[_train])\n",
        "        local_prediction = model.predict(X_train[_test])\n",
        "        score_acc = accuracy_score(local_prediction, y_train[_test])\n",
        "        score_f1 = f1_score(local_prediction, y_train[_test])\n",
        "        scores.append(score_acc)\n",
        "        cf = confusion_matrix(local_prediction, y_train[_test])\n",
        "        print(f\"폴드: {k + 1}, 클래스 분포: {np.bincount(y_train[_test])}, 정확도: {score_acc:.4f}, f1: {score_f1:.4f}\")\n",
        "        print(cf)\n",
        "    print(\"CV 정확도: %.4f +/- %.4f\" % (np.mean(scores), np.std(scores)))\n",
        "\n",
        "    # scores = cross_validate(\n",
        "    #     estimator=model,\n",
        "    #     X=X_train,\n",
        "    #     y=y_train,\n",
        "    #     scoring=[\"accuracy\", \"f1\"],\n",
        "    #     cv=10,\n",
        "    #     n_jobs=-1,\n",
        "    #     return_train_score=False\n",
        "    # )\n",
        "\n",
        "    # scores = cross_val_score(\n",
        "    #     estimator=model,\n",
        "    #     X=X_train,\n",
        "    #     y=y_train,\n",
        "    #     cv=10,\n",
        "    #     n_jobs=1\n",
        "    # )\n",
        "    # print(\"CV 정확도 점수: %s\" % scores)"
      ],
      "metadata": {
        "id": "hDhzKpe_7gKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **data_processor.py**\n",
        "1. 프로젝트에서 사용되는 머신 러닝 모델들을 따로 모아둔 라이브러리와 모델의 성능을 세밀히 테스트하는 라이브러리를 구축하였습니다.\n",
        "2. k-fold 교차 검증을 위해 두개의 훈련, 검증 csv 파일을 하나의 json 파일로 통합하는 코드를 추가하였습니다.\n",
        "3. 피처 해싱을 사용했습니다.\n",
        "4. 학습데이터와 검증데이터를 나누어 처리하였습니다.\n",
        "5. PEstudio에서 피쳐를 추출할 수 있게 작성하였습니다.\n"
      ],
      "metadata": {
        "id": "mi_JMMMm86ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import glob\n",
        "import copy\n",
        "import os.path\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "\n",
        "\n",
        "def read_label_csv(path):\n",
        "    label_table = dict()\n",
        "    with open(path, \"r\", encoding='ISO-8859-1') as f:\n",
        "        for line in f.readlines()[1:]:\n",
        "            fname, label = line.strip().split(\",\")\n",
        "            label_table[fname] = int(label)\n",
        "    return label_table\n",
        "\n",
        "\n",
        "def read_json(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def parse_set(path):\n",
        "    ret = set()\n",
        "    f = open(path, \"r\")\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "    for line in lines:\n",
        "        ret.add(line.rstrip())\n",
        "    return ret\n",
        "\n",
        "\n",
        "class PeminerParser:\n",
        "    def __init__(self, path):\n",
        "        self.report = read_json(path)\n",
        "        self.vector = []\n",
        "\n",
        "    def hashing(self):\n",
        "        h = FeatureHasher(n_features=188)\n",
        "        hash_dict = dict()\n",
        "\n",
        "        for i in self.report.items():\n",
        "            hash_dict[i[0]] = i[1]\n",
        "        digest = h.transform([hash_dict])\n",
        "        return digest.toarray().tolist()[0]\n",
        "\n",
        "    def process_report(self):\n",
        "        # 전체 데이터 사용 (그런데 피처 해싱을 곁들인)\n",
        "        self.vector += self.hashing()\n",
        "        return self.vector\n",
        "\n",
        "\n",
        "class EmberParser:\n",
        "    '''\n",
        "        예제에서 사용하지 않은 특징도 사용하여 벡터화 할 것을 권장\n",
        "    '''\n",
        "\n",
        "    def __init__(self, path):\n",
        "        self.report = read_json(path)\n",
        "        self.vector = []\n",
        "\n",
        "    def get_histogram_info(self):\n",
        "        histogram = np.array(self.report[\"histogram\"])\n",
        "        total = histogram.sum()\n",
        "        vector = histogram / total\n",
        "        return vector.tolist()\n",
        "\n",
        "    def get_string_info(self):\n",
        "        strings = self.report[\"strings\"]\n",
        "\n",
        "        hist_divisor = float(strings['printables']) if strings['printables'] > 0 else 1.0\n",
        "        vector = [\n",
        "            strings['numstrings'],\n",
        "            strings['avlength'],\n",
        "            strings['printables'],\n",
        "            strings['entropy'],\n",
        "            strings['paths'],\n",
        "            strings['urls'],\n",
        "            strings['registry'],\n",
        "            strings['MZ']\n",
        "        ]\n",
        "        vector += (np.asarray(strings['printabledist']) / hist_divisor).tolist()\n",
        "        return vector\n",
        "\n",
        "    def get_general_file_info(self):\n",
        "        general = self.report[\"general\"]\n",
        "        vector = [\n",
        "            general['size'], general['vsize'], general['has_debug'], general['exports'], general['imports'],\n",
        "            general['has_relocations'], general['has_resources'], general['has_signature'], general['has_tls'],\n",
        "            general['symbols']\n",
        "        ]\n",
        "        return vector\n",
        "\n",
        "    def get_coff_info(self):\n",
        "        if self.report[\"header\"][\"coff\"][\"timestamp\"] == 0:\n",
        "            return [1]\n",
        "        else:\n",
        "            return [0]\n",
        "\n",
        "    def get_data_directories_info(self):\n",
        "        # except runtime header\n",
        "        dd = self.report[\"datadirectories\"]\n",
        "        val1 = 0\n",
        "        for i in dd:\n",
        "            if i[\"name\"] == \"CLR_RUNTIME_HEADER\":\n",
        "                val1 += i[\"virtual_address\"]\n",
        "        return [val1 / max(1, len(dd))]\n",
        "\n",
        "    def get_clr_runtime_header_info(self):\n",
        "        dd = self.report[\"datadirectories\"]\n",
        "        val1 = 0\n",
        "        val2 = 0\n",
        "        for i in dd:\n",
        "            if i[\"name\"] == \"CLR_RUNTIME_HEADER\":\n",
        "                val1 = i[\"virtual_address\"]\n",
        "                val2 = i[\"size\"]\n",
        "        return [val1, val2]\n",
        "\n",
        "    def get_section_info(self):\n",
        "        sections = self.report[\"section\"][\"sections\"]\n",
        "        h = FeatureHasher(n_features=24)\n",
        "        feat_length = len(sections)\n",
        "        numerical_val = [0, 0, 0]\n",
        "        numerical_arg = [\"size\", \"entropy\", \"vsize\"]\n",
        "        ret = []\n",
        "        hash_props = dict()\n",
        "\n",
        "        for i in sections:\n",
        "            for j in range(3):\n",
        "                numerical_val[j] += i[numerical_arg[j]]\n",
        "\n",
        "            for j in i[\"props\"]:\n",
        "                if j not in hash_props:\n",
        "                    hash_props[j] = 1\n",
        "                else:\n",
        "                    hash_props[j] += 1\n",
        "            hash_props[i[\"name\"]] = 1\n",
        "\n",
        "        for i in range(3):\n",
        "            numerical_val[i] /= max(1, feat_length)\n",
        "\n",
        "        digest = h.transform([hash_props])\n",
        "        return ret + digest.toarray().tolist()[0]\n",
        "\n",
        "    def get_imports_title_info(self):\n",
        "        feats = 27\n",
        "        ret = np.zeros(feats)\n",
        "\n",
        "        imports = self.report[\"imports\"]\n",
        "        h = FeatureHasher(n_features=feats, input_type=\"string\")\n",
        "        hash_list = []\n",
        "        for i in imports.keys():\n",
        "            hash_list.append(i)\n",
        "\n",
        "        if len(hash_list) == 0:\n",
        "            return ret.tolist()\n",
        "\n",
        "        digest = h.transform(hash_list).toarray()\n",
        "        for i in range(digest.shape[0]):\n",
        "            ret = np.add(ret, digest[i])\n",
        "        return ret.tolist()\n",
        "\n",
        "    def process_report(self):\n",
        "        vector = []\n",
        "        vector += self.get_general_file_info()\n",
        "        vector += self.get_histogram_info()\n",
        "        vector += self.get_string_info()\n",
        "        vector += self.get_clr_runtime_header_info()\n",
        "        vector += self.get_coff_info()\n",
        "        vector += self.get_section_info()\n",
        "        vector += self.get_imports_title_info()\n",
        "        '''\n",
        "            특징 추가\n",
        "        '''\n",
        "        return vector\n",
        "\n",
        "\n",
        "class PestudioParser:\n",
        "    '''\n",
        "        사용할 특징을 선택하여 벡터화 할 것을 권장\n",
        "    '''\n",
        "\n",
        "    def __init__(self, path):\n",
        "        self.na = \"n/a\"\n",
        "        self.report = read_json(path)\n",
        "        self.vector = []\n",
        "        self.severity_tags = parse_set(\"util_pestudio/severity_tag.txt\")\n",
        "        self.mal_api1 = parse_set(\"util_pestudio/API_used_in_Malware_1.txt\")\n",
        "        self.mal_api2 = parse_set(\"util_pestudio/API_used_in_Malware_2.txt\")\n",
        "\n",
        "    def get_indicator_vector(self):\n",
        "        ret = [0, 0, 0]\n",
        "\n",
        "        indicators = self.report[\"image\"][\"indicators\"]\n",
        "        if indicators[\"@hint\"] == self.na or indicators[\"@hint\"] == '0':\n",
        "            return [0, 0, 0]\n",
        "\n",
        "        for i in indicators[\"indicator\"]:\n",
        "            if i[\"@xml-id\"] in self.severity_tags:\n",
        "                ret[0] += 1\n",
        "            if int(i[\"@severity\"]) == 1:\n",
        "                ret[1] += 1\n",
        "            elif int(i[\"@severity\"]) == 2:\n",
        "                ret[2] += 1\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def get_compiler_stamp_info(self):\n",
        "        if \"file-header\" not in self.report[\"image\"]:\n",
        "            return [0]\n",
        "\n",
        "        token = self.report[\"image\"][\"file-header\"][\"compiler-stamp\"]\n",
        "        if int(token.split(\" \")[-1]) > 2018:\n",
        "            return [1]\n",
        "        else:\n",
        "            return [0]\n",
        "\n",
        "    def get_seh_info(self):\n",
        "        if \"optional-header\" not in self.report[\"image\"]:\n",
        "            return [0]\n",
        "\n",
        "        val = self.report[\"image\"][\"optional-header\"][\"structured-exception-handling\"]\n",
        "        if val == \"true\":\n",
        "            return [1]\n",
        "        else:\n",
        "            return [0]\n",
        "\n",
        "    def get_section_info(self):\n",
        "        ret = [0, 0, 0]\n",
        "        if \"sections\" not in self.report[\"image\"]:\n",
        "            return ret\n",
        "\n",
        "        token = self.report[\"image\"][\"sections\"][\"section\"]\n",
        "        if isinstance(token, dict):\n",
        "            if token[\"@name\"] and token[\"@writable\"] == 'x':\n",
        "                ret[0] += 1\n",
        "            if token[\"@name\"] == \".rsrc\":\n",
        "                if token[\"@entropy\"] != self.na:\n",
        "                    ret[1] = float(token[\"@entropy\"])\n",
        "                if token[\"@file-ratio\"] != self.na:\n",
        "                    ret[2] = float(token[\"@file-ratio\"])\n",
        "        else:\n",
        "            for i in token:\n",
        "                if i[\"@name\"] == \".text\" and i[\"@writable\"] == \"x\":\n",
        "                    ret[0] += 1\n",
        "                if i[\"@name\"] == \".rsrc\":\n",
        "                    if i[\"@entropy\"] != self.na:\n",
        "                        ret[1] = float(i[\"@entropy\"])\n",
        "\n",
        "                    if i[\"@file-ratio\"] != self.na:\n",
        "                        ret[2] = float(i[\"@file-ratio\"])\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def count_libraries_blacklist(self):\n",
        "        if \"libraries\" not in self.report[\"image\"]:\n",
        "            return [0]\n",
        "\n",
        "        ret = 0\n",
        "\n",
        "        token = self.report[\"image\"][\"libraries\"]\n",
        "        if token[\"@hint\"] == self.na or \"library\" not in token:\n",
        "            return [0]\n",
        "        elif isinstance(token[\"library\"], dict):\n",
        "            if token[\"library\"][\"@blacklist\"] == 'x':\n",
        "                ret += 1\n",
        "        else:\n",
        "            for i in token[\"library\"]:\n",
        "                if i[\"@blacklist\"] == \"x\":\n",
        "                    ret += 1\n",
        "        return [ret]\n",
        "\n",
        "    def get_imports_vector(self):\n",
        "        mal_set = self.mal_api1 | self.mal_api2\n",
        "        # import_count = 0\n",
        "        blacklist = 0\n",
        "        malicious = 0\n",
        "\n",
        "        if \"imports\" not in self.report[\"image\"] or \"import\" not in self.report[\"image\"][\"imports\"]:\n",
        "            return [0, 0]\n",
        "\n",
        "        token = self.report[\"image\"][\"imports\"][\"import\"]\n",
        "\n",
        "        if isinstance(token, dict):\n",
        "            if token[\"@blacklist\"] == 'x':\n",
        "                blacklist = 1\n",
        "            if token[\"@name\"] in mal_set:\n",
        "                malicious = 1\n",
        "        else:\n",
        "            for i in token:\n",
        "                if i[\"@blacklist\"] == 'x':\n",
        "                    blacklist += 1\n",
        "\n",
        "                if i[\"@name\"] not in mal_set:\n",
        "                    continue\n",
        "                else:\n",
        "                    malicious += 1\n",
        "\n",
        "        return [malicious, blacklist]\n",
        "\n",
        "    def get_tls_callback_info(self):\n",
        "        if \"tls-callbacks\" not in self.report[\"image\"]:\n",
        "            return [0]\n",
        "\n",
        "        upx_w = 1\n",
        "        null_w = 0.5\n",
        "        tls = self.report[\"image\"][\"tls-callbacks\"]\n",
        "        if tls == self.na or tls[\"@hint\"] == self.na:\n",
        "            return [0]\n",
        "\n",
        "        val = int(tls[\"@hint\"])\n",
        "        ret = 0\n",
        "\n",
        "        if val == 1:\n",
        "            if tls[\"callback\"] is None:\n",
        "                return [null_w]\n",
        "\n",
        "            token = tls[\"callback\"][\"@section\"].lower()\n",
        "            if token.startswith(\"upx\"):\n",
        "                ret += upx_w\n",
        "            else:\n",
        "                ret += 0.1\n",
        "        else:\n",
        "            for i in tls[\"callback\"]:\n",
        "                if i is None:\n",
        "                    ret += null_w\n",
        "                    continue\n",
        "\n",
        "                token = i[\"@section\"].lower()\n",
        "\n",
        "                if token.startswith(\"upx\"):\n",
        "                    ret += upx_w\n",
        "                else:\n",
        "                    ret += 0.1\n",
        "\n",
        "        return [ret]\n",
        "\n",
        "    def get_certification_info(self):\n",
        "        if \"certificate\" not in self.report[\"image\"]:\n",
        "            return [0]\n",
        "\n",
        "        if self.report[\"image\"][\"certificate\"] == self.na:\n",
        "            return [0.5]\n",
        "        if self.report[\"image\"][\"certificate\"] == \"expired\":\n",
        "            return [1]\n",
        "        else:\n",
        "            return [0]\n",
        "\n",
        "    def process_report(self):\n",
        "        ret = []\n",
        "        ret += self.get_indicator_vector()\n",
        "        ret += self.get_section_info()\n",
        "        ret += self.get_compiler_stamp_info()\n",
        "        ret += self.get_seh_info()\n",
        "        ret += self.count_libraries_blacklist()\n",
        "        ret += self.get_imports_vector()\n",
        "        ret += self.get_tls_callback_info()\n",
        "        ret += self.get_certification_info()\n",
        "        return ret\n",
        "\n",
        "\n",
        "def get_train_path(petype: str):\n",
        "    checker = set()\n",
        "    checker.add(\"EMBER\")\n",
        "    checker.add(\"PEMINER\")\n",
        "    checker.add(\"PESTUDIO\")\n",
        "    if petype not in checker:\n",
        "        print(\"Invalid path!\")\n",
        "        return\n",
        "\n",
        "    return f\"데이터/{petype}/학습데이터/\"\n",
        "\n",
        "\n",
        "def get_valid_path(petype: str):\n",
        "    checker = set()\n",
        "    checker.add(\"EMBER\")\n",
        "    checker.add(\"PEMINER\")\n",
        "    checker.add(\"PESTUDIO\")\n",
        "    if petype not in checker:\n",
        "        print(\"Invalid path!\")\n",
        "        return\n",
        "\n",
        "    return f\"데이터/{petype}/검증데이터/\"\n",
        "\n",
        "\n",
        "def get_test_path(petype: str):\n",
        "    checker = set()\n",
        "    checker.add(\"EMBER\")\n",
        "    checker.add(\"PEMINER\")\n",
        "    checker.add(\"PESTUDIO\")\n",
        "    if petype not in checker:\n",
        "        print(\"Invalid path!\")\n",
        "        return\n",
        "\n",
        "    return f\"데이터/{petype}/테스트데이터/\"\n",
        "\n",
        "\n",
        "def process_dataset(datatype: str, drop=False):\n",
        "    basepath = \"\"\n",
        "    labels = None\n",
        "\n",
        "    if datatype == \"TRAIN\":\n",
        "        labels = read_label_csv(\"데이터/학습데이터_정답.csv\")\n",
        "        basepath = get_train_path(\"PEMINER\")\n",
        "    elif datatype == \"VALID\":\n",
        "        labels = read_label_csv(\"데이터/검증데이터_정답.csv\")\n",
        "        basepath = get_valid_path(\"PEMINER\")\n",
        "    else:\n",
        "        print(\"레이블이 없는 데이터입니다.\")\n",
        "        return dict()\n",
        "\n",
        "    datadict = dict()\n",
        "\n",
        "    for fname in glob.glob(basepath + '*'):\n",
        "        # Erase \".json\"\n",
        "        key = fname[:-5].split('/')[-1]\n",
        "        # Insert feature vector\n",
        "        datadict[key] = copy.deepcopy(PeminerParser(fname).process_report())\n",
        "\n",
        "    if datatype == \"TRAIN\":\n",
        "        basepath = get_train_path(\"EMBER\")\n",
        "    else:\n",
        "        basepath = get_valid_path(\"EMBER\")\n",
        "\n",
        "    for fname in glob.glob(basepath + '*'):\n",
        "        key = fname[:-5].split('/')[-1]\n",
        "        datadict[key] += copy.deepcopy(EmberParser(fname).process_report())\n",
        "\n",
        "    # TODO: Handle the pestudio\n",
        "    if datatype == \"TRAIN\":\n",
        "        basepath = get_train_path(\"PESTUDIO\")\n",
        "    else:\n",
        "        basepath = get_valid_path(\"PESTUDIO\")\n",
        "\n",
        "    import pestudio_extractor\n",
        "    imputation_vector = pestudio_extractor.read_imputation_vector()\n",
        "\n",
        "    for i in labels.keys():\n",
        "        path = basepath + i + \".json\"\n",
        "        if not os.path.isfile(path):\n",
        "            if drop:\n",
        "                del datadict[i]\n",
        "            else:\n",
        "                datadict[i] += copy.deepcopy(imputation_vector)\n",
        "        else:\n",
        "            datadict[i] += copy.deepcopy(PestudioParser(path).process_report())\n",
        "\n",
        "    ret = {\n",
        "        \"data_names\": [],\n",
        "        \"data\": [],\n",
        "        \"target\": np.array([], dtype=np.uint8)\n",
        "    }\n",
        "    for key in labels.keys():\n",
        "        if key in datadict:\n",
        "            ret[\"data_names\"].append(key)\n",
        "            ret[\"data\"].append(datadict[key])\n",
        "            ret[\"target\"] = np.append(ret[\"target\"], labels[key])\n",
        "    ret[\"data\"] = np.array(ret[\"data\"])\n",
        "    return ret\n",
        "\n",
        "\n",
        "def combine_dataset(data1: dict, data2: dict):\n",
        "    return {\n",
        "        \"data\": np.append(data1[\"data\"], data2[\"data\"], axis=0),\n",
        "        \"target\": np.append(data1[\"target\"], data2[\"target\"]),\n",
        "        \"data_names\": data1[\"data_names\"] + data2[\"data_names\"]\n",
        "    }\n",
        "\n",
        "\n",
        "def write_dataset(path: str, data: dict):\n",
        "    dt = copy.deepcopy(data)\n",
        "    dt[\"data\"] = dt[\"data\"].tolist()\n",
        "    dt[\"target\"] = dt[\"target\"].tolist()\n",
        "    with open(path, 'w', encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(dt))\n",
        "\n",
        "\n",
        "def read_dataset(path: str):\n",
        "    if not os.path.isfile(path):\n",
        "        print(\"Invalid path!!\")\n",
        "        return None\n",
        "\n",
        "    txt = open(path, 'r', encoding=\"utf-8\").read()\n",
        "    ret = json.loads(txt)\n",
        "    ret[\"data\"] = np.array(ret[\"data\"])\n",
        "    ret[\"target\"] = np.array(ret[\"target\"])\n",
        "    return ret"
      ],
      "metadata": {
        "id": "QES_k5ho99kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **live_evaluator.py**\n",
        "1. training.json, validation.json, dataset.json이 있는지 검증하고, 이 json 파일들이 없다면 data_processor.py의 write_dataset 함수로 json 파일을 생성하게 했습니다.\n",
        "2. dataset.json은 training.json과 validation.json이 k-fold 교차 검증을 이용하기 위해 결합된 것입니다.\n",
        "3. k-fold 교차 검증 외에도 기존 홀드아웃 기반 모델 평가를 사용자가 선택할 수 있도록 코드를 개선했습니다.\n"
      ],
      "metadata": {
        "id": "f9hyC4K0CHrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import data_processor\n",
        "import model_lib\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_set = data_processor.read_dataset(\"training.json\")\n",
        "    valid_set = data_processor.read_dataset(\"validation.json\")\n",
        "    combined_set = data_processor.read_dataset(\"dataset.json\")\n",
        "\n",
        "    if train_set is None:\n",
        "        print(\"정제된 훈련 데이터가 없습니다. 새로 생성합니다.\")\n",
        "        train_set = data_processor.process_dataset(\"TRAIN\")\n",
        "        data_processor.write_dataset(\"training.json\", train_set)\n",
        "\n",
        "    if valid_set is None:\n",
        "        print(\"정제된 검증 데이터가 없습니다. 새로 생성합니다.\")\n",
        "        valid_set = data_processor.process_dataset(\"VALID\")\n",
        "        data_processor.write_dataset(\"validation.json\", valid_set)\n",
        "\n",
        "    if combined_set is None:\n",
        "        print(\"정제한 합본 데이터셋이 존재하지 않습니다. 새로 생성합니다.\")\n",
        "        combined_set = data_processor.combine_dataset(\n",
        "            train_set,\n",
        "            valid_set\n",
        "        )\n",
        "        data_processor.write_dataset(\"dataset.json\", combined_set)\n",
        "    combined_X = combined_set[\"data\"]\n",
        "    combined_y = combined_set[\"target\"]\n",
        "\n",
        "    while True:\n",
        "        print(\"다음 중 원하는 평가 방법을 입력\")\n",
        "        print(\"1: holdout validation\")\n",
        "        print(\"2: k-fold cross validation\")\n",
        "        print(\"유효하지 않은 값일 경우 프로세스 종료\")\n",
        "        evaluate_type = input()\n",
        "\n",
        "        if evaluate_type != \"1\" and evaluate_type != \"2\":\n",
        "            print(\"유효하지 않은 값 입력됨. 프로세스 종료\")\n",
        "            break\n",
        "\n",
        "        val = input(\"측정을 원하는 모델을 입력(유효하지 않은 값일 경우 프로세스 종료): \")\n",
        "        model = model_lib.load_model(model=val, random_state=41)\n",
        "        if model is None:\n",
        "            print(\"유효하지 않은 값 입력됨. 프로세스 종료\")\n",
        "            break\n",
        "        # pipe = make_pipeline(\n",
        "        #     StandardScaler(),\n",
        "        #     model\n",
        "        # )\n",
        "\n",
        "        if evaluate_type == \"1\":\n",
        "            model.fit(\n",
        "                train_set[\"data\"],\n",
        "                train_set[\"target\"],\n",
        "            )\n",
        "            model_lib.evaluate(\n",
        "                valid_set[\"data\"],\n",
        "                valid_set[\"target\"],\n",
        "                model\n",
        "            )\n",
        "        else:\n",
        "            model_lib.evaluate_kfold(combined_X, combined_y, model)"
      ],
      "metadata": {
        "id": "9xJUXZk7EdfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**pestudio_extractor.py**\n",
        "\n",
        "1.   PEstudio에서 .json파일 560개가 없어 결측치를 추정하는 벡터를 생성하는 스크립트입니다.\n",
        "2.   mean 또는 mode값으로 결측치를 보관한 벡터를 생성하도록 합니다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JthFqg-tI6Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import copy\n",
        "import data_processor\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "def guess_imputation_vector():\n",
        "    ret = []\n",
        "    pathes = [\n",
        "        data_processor.get_train_path(\"PESTUDIO\"),\n",
        "        data_processor.get_valid_path(\"PESTUDIO\")\n",
        "    ]\n",
        "\n",
        "    for i in pathes:\n",
        "        for fname in glob.glob(i + '*'):\n",
        "            key = fname[:-5].split('/')[-1]\n",
        "            # print(key)\n",
        "            ret.append(copy.deepcopy(data_processor.PestudioParser(fname).process_report()))\n",
        "\n",
        "    ret = np.array(ret)\n",
        "    # return np.mean(ret, axis=0)\n",
        "    return stats.mode(ret)[0].tolist()[0]\n",
        "\n",
        "\n",
        "def write_imputation_vector(val: list):\n",
        "    f = open(\"util_pestudio/imputation.txt\", 'w')\n",
        "    for i in val:\n",
        "        f.write(str(i) + '\\n')\n",
        "    f.close()\n",
        "\n",
        "\n",
        "def read_imputation_vector():\n",
        "    ret = []\n",
        "    f = open(\"util_pestudio/imputation.txt\", 'r')\n",
        "    txt = f.readlines()\n",
        "    for line in txt:\n",
        "        ret.append(float(line.rstrip()))\n",
        "    return ret\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    res = guess_imputation_vector()\n",
        "    print(res)\n",
        "    write_imputation_vector(res)"
      ],
      "metadata": {
        "id": "QZkHog05I4yU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
