"""
===============================================
주의! 이 파일은 절대 커밋하지 마십시오!
이 스크립트에서는 단지 자신이 스터디한 것을 적용해보는 용도로 씁니다!!
===============================================
"""
import copy
import os
import glob
import json
import pprint

import numpy as np

from lightgbm import LGBMClassifier

from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier

from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import RFE

# ==================================


def get_train_path(petype: str):
    checker = set()
    checker.add("EMBER")
    checker.add("PEMINER")
    checker.add("PESTUDIO")
    if petype not in checker:
        print("Invalid path!")
        return

    return f"데이터/{petype}/학습데이터/"


def get_valid_path(petype: str):
    checker = set()
    checker.add("EMBER")
    checker.add("PEMINER")
    checker.add("PESTUDIO")
    if petype not in checker:
        print("Invalid path!")
        return

    return f"데이터/{petype}/검증데이터/"


def get_test_path(petype: str):
    checker = set()
    checker.add("EMBER")
    checker.add("PEMINER")
    checker.add("PESTUDIO")
    if petype not in checker:
        print("Invalid path!")
        return

    return f"데이터/{petype}/테스트데이터/"


def process_dataset(datatype: str):
    basepath = ""
    labels = None

    if datatype == "TRAIN":
        labels = read_label_csv("데이터/학습데이터_정답.csv")
        basepath = get_train_path("PEMINER")
    elif datatype == "VALID":
        labels = read_label_csv("데이터/검증데이터_정답.csv")
        basepath = get_valid_path("PEMINER")
    else:
        print("레이블이 없는 데이터입니다.")
        return dict()

    datadict = dict()

    for fname in glob.glob(basepath + '*'):
        # Erase ".json"
        key = fname[:-5].split('/')[-1]
        # Insert feature vector
        datadict[key] = copy.deepcopy(PeminerParser(fname).process_report())

    if datatype == "TRAIN":
        basepath = get_train_path("EMBER")
    else:
        basepath = get_valid_path("EMBER")

    for fname in glob.glob(basepath + '*'):
        key = fname[:-5].split('/')[-1]
        datadict[key] += EmberParser(fname).process_report()

    # TODO: handle pestudio
    if datatype == "TRAIN":
        basepath = get_train_path("PESTUDIO")
    else:
        basepath = get_valid_path("PESTUDIO")

    ret = dict()
    ret["data_names"] = list()
    ret["data"] = []
    ret["target"] = np.array([], dtype=np.uint8)
    for key in labels.keys():
        ret["data_names"].append(key)
        ret["data"].append(datadict[key])
        ret["target"] = np.append(ret["target"], labels[key])
    ret["data"] = np.array(ret["data"])
    return ret


# ==================================

SEED = 41


def read_label_csv(path):
    label_table = dict()
    with open(path, "r", encoding='ISO-8859-1') as f:
        for line in f.readlines()[1:]:
            fname, label = line.strip().split(",")
            label_table[fname] = int(label)
    return label_table


def read_json(path):
    with open(path, "r") as f:
        return json.load(f)


def load_model(**kwargs):
    if kwargs["model"] == "rf":
        return RandomForestClassifier(random_state=kwargs["random_state"], n_jobs=4)
    elif kwargs["model"] == "dt":
        return DecisionTreeClassifier(random_state=kwargs["random_state"])
    elif kwargs["model"] == "lgb":
        return LGBMClassifier(random_state=kwargs["random_state"])
    elif kwargs["model"] == "svm":
        return SVC(random_state=kwargs["random_state"])
    elif kwargs["model"] == "lr":
        return LogisticRegression(random_state=kwargs["random_state"], n_jobs=-1)
    elif kwargs["model"] == "knn":
        return KNeighborsClassifier(n_jobs=-1)
    elif kwargs["model"] == "adaboost":
        return AdaBoostClassifier(random_state=kwargs["random_state"])
    elif kwargs["model"] == "mlp":
        return MLPClassifier(random_state=kwargs["random_state"])
    else:
        print("Unsupported Algorithm")
        return None


def train(X_train, y_train, model):
    '''
        머신러닝 모델을 선택하여 학습을 진행하는 함수

        :param X_train: 학습할 2차원 리스트 특징벡터
        :param y_train: 학습할 1차원 리스트 레이블 벡터
        :param model: 문자열, 선택할 머신러닝 알고리즘
        :return: 학습된 머신러닝 모델 객체
    '''
    clf = load_model(model=model, random_state=SEED)
    clf.fit(X_train, y_train)
    return clf


def evaluate(X_test, y_test, model):
    '''
        학습된 머신러닝 모델로 검증 데이터를 검증하는 함수

        :param X_test: 검증할 2차원 리스트 특징 벡터
        :param y_test: 검증할 1차원 리스트 레이블 벡터
        :param model: 학습된 머신러닝 모델 객체
    '''
    predict = model.predict(X_test)
    print("정확도", model.score(X_test, y_test))


label_table = read_label_csv("데이터/학습데이터_정답.csv")

ember_path = "데이터/EMBER/학습데이터/000c4ae5e00a1d4de991a9decf9ecbac59ed5582f5972f05b48bc1a1fe57338a.json"
peminer_path = "데이터/PEMINER/학습데이터/000c4ae5e00a1d4de991a9decf9ecbac59ed5582f5972f05b48bc1a1fe57338a.json"

ember_result = read_json(ember_path)
peminer_result = read_json(peminer_path)


# pprint.pprint(ember_result)
# pprint.pprint(peminer_result)

class PeminerParser:
    def __init__(self, path):
        self.report = read_json(path)
        self.vector = []

    def process_report(self):
        '''
            전체 데이터 사용
        '''

        self.vector = [value for _, value in sorted(self.report.items(), key=lambda x: x[0])]
        return self.vector


class EmberParser:
    '''
        예제에서 사용하지 않은 특징도 사용하여 벡터화 할 것을 권장
    '''

    def __init__(self, path):
        self.report = read_json(path)
        self.vector = []

    def get_histogram_info(self):
        histogram = np.array(self.report["histogram"])
        total = histogram.sum()
        vector = histogram / total
        return vector.tolist()

    def get_string_info(self):
        strings = self.report["strings"]

        hist_divisor = float(strings['printables']) if strings['printables'] > 0 else 1.0
        vector = [
            strings['numstrings'],
            strings['avlength'],
            strings['printables'],
            strings['entropy'],
            strings['paths'],
            strings['urls'],
            strings['registry'],
            strings['MZ']
        ]
        vector += (np.asarray(strings['printabledist']) / hist_divisor).tolist()
        return vector

    def get_general_file_info(self):
        general = self.report["general"]
        vector = [
            general['size'], general['vsize'], general['has_debug'], general['exports'], general['imports'],
            general['has_relocations'], general['has_resources'], general['has_signature'], general['has_tls'],
            general['symbols']
        ]
        return vector

    def process_report(self):
        vector = []
        vector += self.get_general_file_info()
        vector += self.get_histogram_info()
        vector += self.get_string_info()
        '''
            특징 추가
        '''
        return vector


class PestudioParser:
    '''
        사용할 특징을 선택하여 벡터화 할 것을 권장
    '''

    def __init__(self, path):
        self.report = read_json(path)
        self.vector = []

    def process_report(self):
        pass


# 데이터의 특징 벡터 모음(2차원 리스트) : X
# 데이터의 레이블 모음(1차원 리스트) : y
X, y = [], []

for fname in ["000c4ae5e00a1d4de991a9decf9ecbac59ed5582f5972f05b48bc1a1fe57338a", "00ed7bc707559e6e63818b2bba0ac6b338ba17d95aea6f0838cbdc40cb9acd94"]:
    feature_vector = []
    label = label_table[fname]
    for data in ["PEMINER", "EMBER"]:
        path = f"데이터/{data}/학습데이터/{fname}.json"
        if data == "PEMINER":
            feature_vector += PeminerParser(path).process_report()
        else:
            feature_vector += EmberParser(path).process_report()
        print(feature_vector)
    # 기본으로 피처 558개를 추출하고 있음(스튜디오 피처 제외)
    # print(len(feature_vector), feature_vector)
    X.append(feature_vector)
    y.append(label)

print(np.asarray(X).shape, np.asarray(y).shape)

# 학습
models = []
for model in ["rf", "lgb"]:
    clf = train(X, y, model)
    models.append(clf)

# 검증
# 실제 검증 시에는 제공한 검증데이터를 검증에 사용해야 함
for model in models:
    evaluate(X, y, model)


def ensemble_result(X, y, models):
    '''
        학습된 모델들의 결과를 앙상블하는 함수

        :param X: 검증할 2차원 리스트 특징 벡터
        :param y: 검증할 1차원 리스트 레이블 벡터
        :param models: 1개 이상의 학습된 머신러닝 모델 객체를 가지는 1차원 리스트
    '''

    # Soft Voting
    # https://devkor.tistory.com/entry/Soft-Voting-%EA%B3%BC-Hard-Voting
    predicts = []
    for i in range(len(X)):
        probs = []
        for model in models:
            prob = model.predict_proba(X)[i][1]
            probs.append(prob)
        predict = 1 if np.mean(probs) >= 0.5 else 0
        predicts.append(predict)

    print("정확도", accuracy_score(y, predicts))


ensemble_result(X, y, models)


def select_feature(X, y, model):
    '''
        주어진 특징 벡터에서 특정 알고리즘 기반 특징 선택

        본 예제에서는 RFE 알고리즘 사용
        https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE.fit_transform

        :param X: 검증할 2차원 리스트 특징 벡터
        :param y: 검증할 1차원 리스트 레이블 벡터
        :param model: 문자열, 특징 선택에 사용할 머신러닝 알고리즘
    '''

    model = load_model(model=model, random_state=SEED)
    rfe = RFE(estimator=model)
    return rfe.fit_transform(X, y)


selected_X = select_feature(X, y, "rf")
new_model = train(selected_X, y, "rf")

print("START PLAYGROUND SECTION")
# process_dataset: TRAIN을 주면 학습용 데이터셋, VALID를 주면 검증용 데이터셋을 가져오게 됩니다.
dataset = process_dataset("TRAIN")
# print(dataset["data"].shape)
# pprint.pprint(dataset["data"])
validset = process_dataset("VALID")
myX = dataset["data"]
myy = dataset["target"]

mymodel = train(myX, myy, "rf")
yourmodel = train(myX, myy, "lgb")
validX = validset["data"]
validy = validset["target"]

evaluate(validX, validy, mymodel)
evaluate(validX, validy, yourmodel)
ensemble_result(validX, validy, models)